{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- loading data set using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, root_dir, data_types):\n",
    "        self.data = {}\n",
    "        for data_type in data_types:\n",
    "            data_path = os.path.join(root_dir, data_type)\n",
    "            image_folder_path = os.path.join(data_path, \"images\")\n",
    "            label_folder_path = os.path.join(data_path, \"labels\", \"json\")\n",
    "            images = []\n",
    "            labels = []\n",
    "            for image_file in os.listdir(image_folder_path):\n",
    "                image_path = os.path.join(image_folder_path, image_file)\n",
    "                json_file = image_file.split('.')[0] + '.json'\n",
    "                label_path = os.path.join(label_folder_path, json_file)\n",
    "                if os.path.exists(label_path):\n",
    "                    images.append(image_path)\n",
    "                    labels.append(label_path)\n",
    "            self.data[data_type] = {\"images\": images, \"labels\": labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        total_images = 0\n",
    "        for data_type in self.data:\n",
    "            total_images += len(self.data[data_type][\"images\"])\n",
    "        return total_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     data_type = None\n",
    "     image_idx = None\n",
    "     for dt, data in self.data.items():\n",
    "        if idx < len(data['images']):\n",
    "            data_type = dt\n",
    "            image_idx = idx\n",
    "            break\n",
    "        else:\n",
    "            idx -= len(data['images'])\n",
    "     if data_type is not None and image_idx is not None:\n",
    "        image_path = self.data[data_type][\"images\"][image_idx]\n",
    "        label_path = self.data[data_type][\"labels\"][image_idx]\n",
    "        return image_path, label_path\n",
    "     else:\n",
    "        raise IndexError(\"Index out of range.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- visualizing some of the labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(data_loader, num_batches):\n",
    "    colors = {\"bin\": (255, 0, 0), \"dolly\": (0, 255, 0), \"jack\": (0, 0, 255)}\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        images, labels = batch\n",
    "        for image_path, label_path in zip(images, labels):\n",
    "            image_cv2 = cv2.imread(image_path)\n",
    "            image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(image_cv2)\n",
    "\n",
    "            with open(label_path, 'r') as f:\n",
    "                labels = json.load(f)\n",
    "\n",
    "            for bbox in labels:\n",
    "                left = bbox[\"Left\"]\n",
    "                top = bbox[\"Top\"]\n",
    "                right = bbox[\"Right\"]\n",
    "                bottom = bbox[\"Bottom\"]\n",
    "                class_name = bbox[\"ObjectClassName\"]\n",
    "                color = colors.get(class_name, (0, 0, 0))\n",
    "            \n",
    "\n",
    "                cv2.rectangle(image_cv2, (left, top), (right, bottom), color, 2)\n",
    "                cv2.putText(image_cv2, class_name, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 4)\n",
    "\n",
    "            plt.imshow(image_cv2)\n",
    "            plt.title(image_path)\n",
    "            plt.show()\n",
    "           \n",
    "\n",
    "\n",
    "root_dir = \"data\"\n",
    "dataset_train = ImageLabelDataset(root_dir, [\"Training\"])\n",
    "dataset_test = ImageLabelDataset(root_dir, [\"Testing\"])\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_batches_to_visualize = 1\n",
    "print(\"Visualizing training images:\")\n",
    "visualize_images(train_loader, num_batches_to_visualize)\n",
    "\n",
    "print(\"Visualizing testing images:\")\n",
    "visualize_images(test_loader, num_batches_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- adding agmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def augment_and_save_images(data_loader, output_dir, num_batches=1):\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break  \n",
    "        augmentation = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "            A.Blur(blur_limit=(3, 7), p=0.5),\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels', 'class_ids']))\n",
    "        \n",
    "        images, labels = batch\n",
    "    \n",
    "        for image_path, label_path in zip(images, labels):\n",
    "            image = cv2.imread(image_path)\n",
    "            filename = os.path.splitext(os.path.basename(image_path))[0] \n",
    "            output_img_path = os.path.join(output_dir, f\"{filename}_a.jpg\") \n",
    "            output_label_path = os.path.join(os.path.dirname(label_path), f\"{filename}_a.json\") \n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    try:\n",
    "                        original_labels = json.load(f)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error: Unable to parse JSON file: {label_path}\")\n",
    "                        continue\n",
    "\n",
    "                bboxes = []\n",
    "                class_labels = []\n",
    "                class_ids = []  # Add class IDs list\n",
    "                for bbox in original_labels:\n",
    "                    try:\n",
    "                        class_name = bbox[\"ObjectClassName\"]\n",
    "                        class_id = bbox[\"ObjectClassId\"]  # Get class ID\n",
    "                        left = bbox[\"Left\"]\n",
    "                        top = bbox[\"Top\"]\n",
    "                        right = bbox[\"Right\"]\n",
    "                        bottom = bbox[\"Bottom\"]\n",
    "                    except KeyError:\n",
    "                        print(f\"Error: Malformed label in file: {label_path}\")\n",
    "                        continue\n",
    "                    bboxes.append([left, top, right, bottom])\n",
    "                    class_labels.append(class_name)\n",
    "                    class_ids.append(class_id)  # Append class ID\n",
    "\n",
    "                augmented = augmentation(image=image, bboxes=bboxes, class_labels=class_labels, class_ids=class_ids)\n",
    "                augmented_image = augmented['image']\n",
    "                augmented_bboxes = augmented['bboxes']\n",
    "                cv2.imwrite(output_img_path, augmented_image)\n",
    "                with open(output_label_path, 'w') as f_out:\n",
    "                    augmented_labels = []\n",
    "                    for bbox, class_id in zip(augmented_bboxes, class_ids):\n",
    "                        augmented_labels.append({\n",
    "                            \"Left\": int(bbox[0]),\n",
    "                            \"Top\": int(bbox[1]),\n",
    "                            \"Right\": int(bbox[2]),\n",
    "                            \"Bottom\": int(bbox[3]),\n",
    "                            \"ObjectClassName\": class_labels[augmented_bboxes.index(bbox)],\n",
    "                            \"ObjectClassId\": class_id  # Include class ID in augmented JSON\n",
    "                        })\n",
    "                    json.dump(augmented_labels, f_out)\n",
    "\n",
    "root_dir = \"data\"\n",
    "output_dir = \"data/Training/images\"\n",
    "dataset_train = ImageLabelDataset(root_dir, [\"Training\"])\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
    "num_batches_to_augment = 1\n",
    "\n",
    "print(\"Augmenting and saving training images:\")\n",
    "augment_and_save_images(train_loader, output_dir, num_batches_to_augment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting to yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def convert_to_yolo(bbox):\n",
    "    x_center = (bbox['Left'] + bbox['Right']) / (2)\n",
    "    y_center = (bbox['Top'] + bbox['Bottom']) / (2)\n",
    "    width = (bbox['Right'] - bbox['Left']) \n",
    "    height = (bbox['Bottom'] - bbox['Top']) \n",
    "    return f\"{bbox['ObjectClassId']} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "\n",
    "input_folder = \"data/Training/labels/json\"\n",
    "\n",
    "output_folder = \"data/Training/labels/yolo\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for json_file in os.listdir(input_folder):\n",
    "    if json_file.endswith('.json'):\n",
    "        with open(os.path.join(input_folder, json_file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        yolo_data = []\n",
    "        for bbox in data:\n",
    "            yolo_data.append(convert_to_yolo(bbox))\n",
    "        output_filename = os.path.splitext(json_file)[0] + '.txt'\n",
    "        with open(os.path.join(output_folder, output_filename), 'w') as f:\n",
    "            f.write('\\n'.join(yolo_data))\n",
    "\n",
    "print(\"Conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def normalize_yolo_labels(image_dir, label_dir):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith((\".jpg\")):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            label_path = os.path.join(label_dir, os.path.splitext(filename)[0] + \".txt\")\n",
    "\n",
    "            if os.path.exists(label_path):\n",
    "                # Read image dimensions\n",
    "                image = cv2.imread(image_path)\n",
    "                image_height, image_width, _ = image.shape\n",
    "\n",
    "                # Read and normalize labels\n",
    "                with open(label_path, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                    normalized_lines = []\n",
    "                    for line in lines:\n",
    "                        class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
    "\n",
    "                        # Normalize bounding box coordinates\n",
    "                        x_center_norm = max(min(x_center / image_width, 1.0), 0.0)\n",
    "                        y_center_norm = max(min(y_center / image_height, 1.0), 0.0)\n",
    "                        box_width_norm = max(min(box_width / image_width, 1.0), 0.0)\n",
    "                        box_height_norm = max(min(box_height / image_height, 1.0), 0.0)\n",
    "\n",
    "                        normalized_line = f\"{int(class_id)} {x_center_norm:.6f} {y_center_norm:.6f} {box_width_norm:.6f} {box_height_norm:.6f}\\n\"\n",
    "                        normalized_lines.append(normalized_line)\n",
    "\n",
    "                # Write normalized labels back to file\n",
    "                with open(label_path, \"w\") as file:\n",
    "                    file.writelines(normalized_lines)\n",
    "\n",
    "# Example usage\n",
    "image_directory = \"data/Training/images\"\n",
    "label_directory = \"data/Training/labels/yolo\"\n",
    "normalize_yolo_labels(image_directory, label_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_directories(base_folder, subfolders):\n",
    "    for folder in subfolders:\n",
    "        os.makedirs(os.path.join(base_folder, folder), exist_ok=True)\n",
    "\n",
    "def copy_files_to_folder(file_list, dest_folder):\n",
    "    for file_path in file_list:\n",
    "        shutil.copy(file_path, dest_folder)\n",
    "\n",
    "def split_dataset(image_folder, label_folder, split_ratio=0.2):\n",
    "    # Get the list of image files\n",
    "    image_files = os.listdir(image_folder)\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    # Shuffle the image files\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Calculate the number of images for validation based on the split ratio\n",
    "    num_val_images = int(split_ratio * num_images)\n",
    "\n",
    "    # Split the image files into training and validation sets\n",
    "    val_image_files = image_files[:num_val_images]\n",
    "    train_image_files = image_files[num_val_images:]\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_dataset = {\n",
    "        \"images\": [os.path.join(image_folder, img) for img in train_image_files],\n",
    "        \"labels\": [os.path.join(label_folder, os.path.splitext(img)[0] + '.txt') for img in train_image_files]\n",
    "    }\n",
    "    val_dataset = {\n",
    "        \"images\": [os.path.join(image_folder, img) for img in val_image_files],\n",
    "        \"labels\": [os.path.join(label_folder, os.path.splitext(img)[0] + '.txt') for img in val_image_files]\n",
    "    }\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Specify the paths to the image and label folders\n",
    "image_folder = \"data/Training/images\"\n",
    "label_folder = \"data/Training/labels/yolo\"\n",
    "\n",
    "# Split the dataset with a 80-20 split ratio (80% training, 20% validation)\n",
    "train_dataset, val_dataset = split_dataset(image_folder, label_folder, split_ratio=0.2)\n",
    "\n",
    "# Create folders for train and validation datasets\n",
    "train_folder = \"data/Training/train\"\n",
    "val_folder = \"data/Training/validation\"\n",
    "create_directories(train_folder, [\"images\", \"labels\"])\n",
    "create_directories(val_folder, [\"images\", \"labels\"])\n",
    "\n",
    "# Copy images and labels to train folder\n",
    "copy_files_to_folder(train_dataset[\"images\"], os.path.join(train_folder, \"images\"))\n",
    "copy_files_to_folder(train_dataset[\"labels\"], os.path.join(train_folder, \"labels\"))\n",
    "\n",
    "# Copy images and labels to validation folder\n",
    "copy_files_to_folder(val_dataset[\"images\"], os.path.join(val_folder, \"images\"))\n",
    "copy_files_to_folder(val_dataset[\"labels\"], os.path.join(val_folder, \"labels\"))\n",
    "\n",
    "# Print the number of images in the training and validation sets\n",
    "print(\"Number of images in training set:\", len(train_dataset[\"images\"]))\n",
    "print(\"Number of images in validation set:\", len(val_dataset[\"images\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating for yolov7 the class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_class_ids(label_dir):\n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(label_dir, filename), \"r\") as file:\n",
    "                lines = file.readlines()\n",
    "                updated_lines = []\n",
    "                for line in lines:\n",
    "                    class_id, *rest = map(float, line.strip().split())\n",
    "                    # Update class IDs\n",
    "                    if class_id == 4:\n",
    "                        updated_class_id = 0\n",
    "                    elif class_id == 5:\n",
    "                        updated_class_id = 1\n",
    "                    elif class_id == 7:\n",
    "                        updated_class_id = 2\n",
    "                    else:\n",
    "                        updated_class_id = class_id\n",
    "                    updated_line = f\"{updated_class_id} {' '.join(map(str, rest))}\\n\"\n",
    "                    updated_lines.append(updated_line)\n",
    "            # Write updated labels back to file\n",
    "            with open(os.path.join(label_dir, filename), \"w\") as file:\n",
    "                file.writelines(updated_lines)\n",
    "\n",
    "# Example usage\n",
    "label_directory = \"data/yolov7-custom/data/train/labels\"\n",
    "update_class_ids(label_directory)\n",
    "label_directory = \"data/yolov7-custom/data/validation/labels\"\n",
    "update_class_ids(label_directory)\n",
    "label_directory = \"data/Training/train/labels\"\n",
    "update_class_ids(label_directory)\n",
    "label_directory = \"data/Training/validation/labels\"\n",
    "update_class_ids(label_directory)\n",
    "label_directory = \"data/Training/labels/yolo\"\n",
    "update_class_ids(label_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --workers 1 --device cpu --batch-size 8 --epochs 20 --img 640 640 --data data/custom_data.yaml --hyp data/hyp.scratch.custom.yaml --cfg cfg/training/yolov7-custom.yaml  --weights yolov7.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exporting yolov7 to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data/yolov7-custom/export.py --weights data/best.pt --img-size 640 --batch-size 1 --dynamic --include-nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!netron  data/best.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my own model form scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# IoU function\n",
    "def iou(box1, box2, is_pred=True):\n",
    "\tif is_pred:\n",
    "\t\tb1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "\t\tb1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "\t\tb1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "\t\tb1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "\n",
    "\t\tb2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "\t\tb2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "\t\tb2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "\t\tb2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "\n",
    "\t\tx1 = torch.max(b1_x1, b2_x1)\n",
    "\t\ty1 = torch.max(b1_y1, b2_y1)\n",
    "\t\tx2 = torch.min(b1_x2, b2_x2)\n",
    "\t\ty2 = torch.min(b1_y2, b2_y2)\n",
    "\t\tintersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "\t\tbox1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n",
    "\t\tbox2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n",
    "\t\tunion = box1_area + box2_area - intersection\n",
    "\t\tepsilon = 1e-6\n",
    "\t\tiou_score = intersection / (union + epsilon)\n",
    "\t\treturn iou_score\n",
    "\n",
    "\telse:\n",
    "\t\t\n",
    "\t\tintersection_area = torch.min(box1[..., 0], box2[..., 0]) * \\\n",
    "\t\t\t\t\t\t\ttorch.min(box1[..., 1], box2[..., 1])\n",
    "\t\tbox1_area = box1[..., 0] * box1[..., 1]\n",
    "\t\tbox2_area = box2[..., 0] * box2[..., 1]\n",
    "\t\tunion_area = box1_area + box2_area - intersection_area\n",
    "\t\tiou_score = intersection_area / union_area\n",
    "\t\treturn iou_score\n",
    "\n",
    "# non-maximum suppression function to remove overlapping bounding boxes\n",
    "def nms(bboxes, iou_threshold, threshold):\n",
    "\tbboxes = [box for box in bboxes if box[1] > threshold]\n",
    "\tbboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\tbboxes_nms = []\n",
    "\n",
    "\twhile bboxes:\n",
    "\t\tfirst_box = bboxes.pop(0)\n",
    "\t\tfor box in bboxes:\n",
    "\t\t\tif box[0] != first_box[0] or iou(\n",
    "\t\t\t\ttorch.tensor(first_box[2:]),\n",
    "\t\t\t\ttorch.tensor(box[2:]),\n",
    "\t\t\t) < iou_threshold:\n",
    "\t\t\t\tif box not in bboxes_nms:\n",
    "\t\t\t\t\tbboxes_nms.append(box)\n",
    "\treturn bboxes_nms\n",
    "\n",
    "#to convert cells to bounding boxes\n",
    "def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True):\n",
    "\tbatch_size = predictions.shape[0]\n",
    "\tnum_anchors = len(anchors)\n",
    "\tbox_predictions = predictions[..., 1:5]\n",
    "\n",
    "\tif is_predictions:\n",
    "\t\tanchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
    "\t\tbox_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "\t\tbox_predictions[..., 2:] = torch.exp(\n",
    "\t\t\tbox_predictions[..., 2:]) * anchors\n",
    "\t\tscores = torch.sigmoid(predictions[..., 0:1])\n",
    "\t\tbest_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
    "\telse:\n",
    "\t\tscores = predictions[..., 0:1]\n",
    "\t\tbest_class = predictions[..., 5:6]\n",
    "\tcell_indices = (\n",
    "\t\ttorch.arange(s)\n",
    "\t\t.repeat(predictions.shape[0], 3, s, 1)\n",
    "\t\t.unsqueeze(-1)\n",
    "\t\t.to(predictions.device)\n",
    "\t)\n",
    "\n",
    "\tx = 1 / s * (box_predictions[..., 0:1] + cell_indices)\n",
    "\ty = 1 / s * (box_predictions[..., 1:2] +\n",
    "\t\t\t\tcell_indices.permute(0, 1, 3, 2, 4))\n",
    "\twidth_height = 1 / s * box_predictions[..., 2:4]\n",
    "\tconverted_bboxes = torch.cat(\n",
    "\t\t(best_class, scores, x, y, width_height), dim=-1\n",
    "\t).reshape(batch_size, num_anchors * s * s, 6)\n",
    "\n",
    "\treturn converted_bboxes.tolist()\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "\tcolour_map = plt.get_cmap(\"tab20b\")\n",
    "\tcolors = [colour_map(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "\timg = np.array(image)\n",
    "\th, w, _ = img.shape\n",
    "\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.imshow(img)\n",
    "\tfor box in boxes:\n",
    "\t\tclass_pred = box[0]\n",
    "\t\tbox = box[2:]\n",
    "\t\tupper_left_x = box[0] - box[2] / 2\n",
    "\t\tupper_left_y = box[1] - box[3] / 2\n",
    "\t\trect = patches.Rectangle(\n",
    "\t\t\t(upper_left_x * w, upper_left_y * h),\n",
    "\t\t\tbox[2] * w,\n",
    "\t\t\tbox[3] * h,\n",
    "\t\t\tlinewidth=2,\n",
    "\t\t\tedgecolor=colors[int(class_pred)],\n",
    "\t\t\tfacecolor=\"none\",\n",
    "\t\t)\n",
    "\t\tax.add_patch(rect)\n",
    "\t\tplt.text(\n",
    "\t\t\tupper_left_x * w,\n",
    "\t\t\tupper_left_y * h,\n",
    "\t\t\ts=class_labels[int(class_pred)],\n",
    "\t\t\tcolor=\"white\",\n",
    "\t\t\tverticalalignment=\"top\",\n",
    "\t\t\tbbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "\t\t)\n",
    "\tplt.show()\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "\tprint(\"==> Saving checkpoint\")\n",
    "\tcheckpoint = {\n",
    "\t\t\"state_dict\": model.state_dict(),\n",
    "\t\t\"optimizer\": optimizer.state_dict(),\n",
    "\t}\n",
    "\ttorch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "\tprint(\"==> Loading checkpoint\")\n",
    "\tcheckpoint = torch.load(checkpoint_file, map_location=device)\n",
    "\tmodel.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\toptimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group[\"lr\"] = lr\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "load_model = False\n",
    "save_model = True\n",
    "checkpoint_file = \"checkpoint.pth.tar\"\n",
    "ANCHORS = [\n",
    "\t[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "\t[(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "\t[(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "batch_size = 32\n",
    "leanring_rate = 1e-5\n",
    "epochs = 20\n",
    "image_size = 416\n",
    "s = [image_size // 32, image_size // 16, image_size // 8]\n",
    "\n",
    "class_labels = [\n",
    "\t\"bin\",\"dolly\",\"jack\"\n",
    "]\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, image_dir, label_dir, anchors,\n",
    "        image_size=416, grid_sizes=[13, 26, 52],\n",
    "        num_classes=3, transform=None\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.grid_sizes = grid_sizes\n",
    "        self.anchors = torch.tensor(\n",
    "            anchors[0] + anchors[1] + anchors[2])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        data = {}\n",
    "        image_files = os.listdir(self.image_dir)\n",
    "        label_files = [filename.split('.')[0] + '.txt' for filename in image_files]\n",
    "\n",
    "        label_files = [file for file in label_files if self.is_valid_label_file(file)]\n",
    "\n",
    "        data['train'] = {'images': [os.path.join(self.image_dir, img) for img in image_files],\n",
    "                         'labels': [os.path.join(self.label_dir, label) for label in label_files]}\n",
    "        return data\n",
    "\n",
    "    def is_valid_label_file(self, file_name):\n",
    "        if '(' in file_name or ')' in file_name:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        total_images = 0\n",
    "        for data_type in self.data:\n",
    "            total_images += len(self.data[data_type][\"images\"])\n",
    "        return total_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self.data['train']['labels']):\n",
    "            print(f\"Index {idx} is out of range for label list. Returning None.\")\n",
    "            return None\n",
    "\n",
    "        label_path = self.data['train']['labels'][idx]\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = self.data['train']['images'][idx]\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            image, bboxes = self.transform(image, bboxes)\n",
    "\n",
    "        targets = [torch.zeros((self.num_anchors_per_scale, s, s, 6)) for s in self.grid_sizes]\n",
    "\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors, is_pred=False)\n",
    "\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            class_label, x, y, width, height = box\n",
    "\n",
    "            has_anchor = [False] * 3\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "\n",
    "                s = self.grid_sizes[scale_idx]\n",
    "                i, j = min(int(s * y), s - 1), min(int(s * x), s - 1)\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = s * x - j, s * y - i\n",
    "                    width_cell, height_cell = (width * s, height * s)\n",
    "\n",
    "                    box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "        return image, tuple(targets)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = [item for item in batch if item is not None]\n",
    "        return default_collate(batch)\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, use_batch_norm=True, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv = nn.Conv2d(in_channels, out_channels, bias=not use_batch_norm, **kwargs)\n",
    "\t\tself.bn = nn.BatchNorm2d(out_channels)\n",
    "\t\tself.activation = nn.LeakyReLU(0.1)\n",
    "\t\tself.use_batch_norm = use_batch_norm\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv(x)\n",
    "\t\tif self.use_batch_norm:\n",
    "\t\t\tx = self.bn(x)\n",
    "\t\t\treturn self.activation(x)\n",
    "\t\telse:\n",
    "\t\t\treturn x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\tdef __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tres_layers = []\n",
    "\t\tfor _ in range(num_repeats):\n",
    "\t\t\tres_layers += [\n",
    "\t\t\t\tnn.Sequential(\n",
    "\t\t\t\t\tnn.Conv2d(channels, channels // 2, kernel_size=1),\n",
    "\t\t\t\t\tnn.BatchNorm2d(channels // 2),\n",
    "\t\t\t\t\tnn.LeakyReLU(0.1),\n",
    "\t\t\t\t\tnn.Conv2d(channels // 2, channels, kernel_size=3, padding=1),\n",
    "\t\t\t\t\tnn.BatchNorm2d(channels),\n",
    "\t\t\t\t\tnn.LeakyReLU(0.1)\n",
    "\t\t\t\t)\n",
    "\t\t\t]\n",
    "\t\tself.layers = nn.ModuleList(res_layers)\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.num_repeats = num_repeats\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tresidual = x\n",
    "\t\t\tx = layer(x)\n",
    "\t\t\tif self.use_residual:\n",
    "\t\t\t\tx = x + residual\n",
    "\t\treturn x\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "\tdef __init__(self, in_channels, num_classes):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.pred = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "\t\t\tnn.BatchNorm2d(2*in_channels),\n",
    "\t\t\tnn.LeakyReLU(0.1),\n",
    "\t\t\tnn.Conv2d(2*in_channels, (num_classes + 5) * 3, kernel_size=1),\n",
    "\t\t)\n",
    "\t\tself.num_classes = num_classes\n",
    "\tdef forward(self, x):\n",
    "\t\toutput = self.pred(x)\n",
    "\t\toutput = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))\n",
    "\t\toutput = output.permute(0, 1, 3, 4, 2)\n",
    "\t\treturn output\n",
    "\n",
    "class my_model(nn.Module):\n",
    "\tdef __init__(self, in_channels=3, num_classes=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_classes = num_classes\n",
    "\t\tself.in_channels = in_channels\n",
    "\n",
    "\t\tself.layers = nn.ModuleList([\n",
    "\t\t\tCNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\tCNNBlock(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tResidualBlock(64, num_repeats=1),\n",
    "\t\t\tCNNBlock(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tResidualBlock(128, num_repeats=2),\n",
    "\t\t\tCNNBlock(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tResidualBlock(256, num_repeats=8),\n",
    "\t\t\tCNNBlock(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tResidualBlock(512, num_repeats=8),\n",
    "\t\t\tCNNBlock(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tResidualBlock(1024, num_repeats=4),\n",
    "\t\t\tCNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tCNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\tResidualBlock(1024, use_residual=False, num_repeats=1),\n",
    "\t\t\tCNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tScalePrediction(512, num_classes=num_classes),\n",
    "\t\t\tCNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tnn.Upsample(scale_factor=2),\n",
    "\t\t\tCNNBlock(768, 256, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tCNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\tResidualBlock(512, use_residual=False, num_repeats=1),\n",
    "\t\t\tCNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tScalePrediction(256, num_classes=num_classes),\n",
    "\t\t\tCNNBlock(256, 128, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tnn.Upsample(scale_factor=2),\n",
    "\t\t\tCNNBlock(384, 128, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tCNNBlock(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\tResidualBlock(256, use_residual=False, num_repeats=1),\n",
    "\t\t\tCNNBlock(256, 128, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\tScalePrediction(128, num_classes=num_classes)\n",
    "\t\t])\n",
    "\tdef forward(self, x):\n",
    "\t\toutputs = []\n",
    "\t\troute_connections = []\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tif isinstance(layer, ScalePrediction):\n",
    "\t\t\t\toutputs.append(layer(x))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tx = layer(x)\n",
    "\t\t\tif isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "\t\t\t\troute_connections.append(x)\n",
    "\t\t\telif isinstance(layer, nn.Upsample):\n",
    "\t\t\t\tx = torch.cat([x, route_connections[-1]], dim=1)\n",
    "\t\t\t\troute_connections.pop()\n",
    "\t\treturn outputs\n",
    "\n",
    "class loss_c(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.mse = nn.MSELoss()\n",
    "\t\tself.bce = nn.BCEWithLogitsLoss()\n",
    "\t\tself.cross_entropy = nn.CrossEntropyLoss()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, pred, target, anchors):\n",
    "\t\tobj = target[..., 0] == 1\n",
    "\t\tno_obj = target[..., 0] == 0\n",
    "\t\tno_object_loss = self.bce(\n",
    "\t\t\t(pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "\t\t)\n",
    "\n",
    "\t\tanchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "\t\tbox_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "\t\t\t\t\t\t\ttorch.exp(pred[..., 3:5]) * anchors\n",
    "\t\t\t\t\t\t\t],dim=-1)\n",
    "\t\tious = iou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "\t\tobject_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]),\n",
    "\t\t\t\t\t\t\tious * target[..., 0:1][obj])\n",
    "\n",
    "\n",
    "\t\tpred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "\t\ttarget[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors)\n",
    "\t\tbox_loss = self.mse(pred[..., 1:5][obj],\n",
    "\t\t\t\t\t\t\ttarget[..., 1:5][obj])\n",
    "\t\tclass_loss = self.cross_entropy((pred[..., 5:][obj]),\n",
    "\t\t\t\t\t\t\t\ttarget[..., 5][obj].long())\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tbox_loss\n",
    "\t\t\t+ object_loss\n",
    "\t\t\t+ no_object_loss\n",
    "\t\t\t+ class_loss\n",
    "\t\t)\n",
    "\n",
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "\tprogress_bar = tqdm(loader, leave=True)\n",
    "\n",
    "\tlosses = []\n",
    "\n",
    "\tfor _, (x, y) in enumerate(progress_bar):\n",
    "\t\tx = x.to(device)\n",
    "\t\ty0, y1, y2 = (\n",
    "\t\t\ty[0].to(device),\n",
    "\t\t\ty[1].to(device),\n",
    "\t\t\ty[2].to(device),\n",
    "\t\t)\n",
    "\n",
    "\t\twith torch.cuda.amp.autocast():\n",
    "\t\t\toutputs = model(x)\n",
    "\t\t\tloss = (\n",
    "\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0])\n",
    "\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1])\n",
    "\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2])\n",
    "\t\t\t)\n",
    "\t\tlosses.append(loss.item())\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tscaler.scale(loss).backward()\n",
    "\t\tscaler.step(optimizer)\n",
    "\t\tscaler.update()\n",
    "\t\tmean_loss = sum(losses) / len(losses)\n",
    "\t\tprogress_bar.set_postfix(loss=mean_loss)\n",
    "  \n",
    "class CustomTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "    def __call__(self, image, bboxes):\n",
    "        transformed_image = self.transform(image)\n",
    "        return transformed_image, bboxes\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), \n",
    "    transforms.Resize((416, 416)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "model = my_model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate)\n",
    "loss_fn = loss_c()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "custom_transform = CustomTransform(transform)\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    image_dir=\"/content/drive/MyDrive/data/Training/images\",\n",
    "    label_dir=\"/content/drive/MyDrive/yolo\",\n",
    "    anchors=ANCHORS,\n",
    "    transform=custom_transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size = batch_size,\n",
    "\tnum_workers = 2,\n",
    "\tshuffle = True,\n",
    "\tpin_memory = True,\n",
    "  collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "scaled_anchors = (\n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n",
    ").to(device)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for e in range(1, epochs+1):\n",
    "    print(\"Epoch:\", e)\n",
    "    training_loss = training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "    writer.add_scalar('Training Loss', training_loss, e)\n",
    "    if save_model:\n",
    "        save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "\n",
    "model = my_model().to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "loss_fn = loss_c() \n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "if load_model: \n",
    "\tload_checkpoint(checkpoint_file, model, optimizer, leanring_rate) \n",
    "\n",
    "test_dataset = Dataset(\n",
    "\timage_dir=\"data/validation/images\", \n",
    "\tlabel_dir=\"data/validation/labels\", \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=custom_transform \n",
    ") \n",
    "test_loader = torch.utils.data.DataLoader( \n",
    "\ttest_dataset, \n",
    "\tbatch_size = 1, \n",
    "\tnum_workers = 2, \n",
    "\tshuffle = True, \n",
    ") \n",
    "\n",
    "x, y = next(iter(test_loader)) \n",
    "x = x.to(device) \n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad(): \n",
    "\toutput = model(x) \n",
    "\tbboxes = [[] for _ in range(x.shape[0])] \n",
    "\tanchors = ( \n",
    "\t\t\ttorch.tensor(ANCHORS) \n",
    "\t\t\t\t* torch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2) \n",
    "\t\t\t).to(device) \n",
    "\tfor i in range(3): \n",
    "\t\tbatch_size, A, S, _, _ = output[i].shape \n",
    "\t\tanchor = anchors[i] \n",
    "\t\tboxes_scale_i = convert_cells_to_bboxes( \n",
    "\t\t\t\t\t\t\toutput[i], anchor, s=S, is_predictions=True\n",
    "\t\t\t\t\t\t) \n",
    "\t\tfor idx, (box) in enumerate(boxes_scale_i): \n",
    "\t\t\tbboxes[idx] += box \n",
    "model.train() \n",
    "\n",
    "for i in range(batch_size): \n",
    "\tnms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)  \n",
    "\tplot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exporting my model to onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Define a sample input tensor (you may need to adjust its size)\n",
    "sample_input = torch.randn(1, 3, 416, 416).to(device)\n",
    "\n",
    "# Export the model to ONNX\n",
    "onnx_file_path = \"my_model.onnx\"\n",
    "torch.onnx.export(model, sample_input, onnx_file_path, verbose=True)\n",
    "\n",
    "print(\"Model exported to:\", onnx_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"torchlogs/\")\n",
    "writer.add_graph(model, X)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/models'\n",
    "files = {'file': open('data/Testing/images/0.jpg', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "print(\"Response content:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/inference'\n",
    "files = {'file': open('data/Testing/images/0.jpg', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/inference_with_overlay'\n",
    "files = {'file': open('data/Testing/images/0.jpg', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "print(\"Response content:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
